{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sentence Window Retriever"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from app.rag import RAGBuilder\n",
    "from app.configs import LLMType, EmbeddingType, IndexType, RetrievalType, IngestionSource, TopicExtractorType\n",
    "from app.evaluation.EvaluateSummary import complete_evaluator, data_loader\n",
    "\n",
    "# Create data generator for evaluation\n",
    "data_generator = data_loader.EvaluationDataGenerator(path=\"EdinburghNLP/xsum\", configuration_name=\"default\")\n",
    "batch = data_generator.generate(num_samples=3)\n",
    "\n",
    "# Build query engine\n",
    "builder = RAGBuilder()\n",
    "sentence_window_query_engine, build_time = (builder\n",
    "                .set_llm_type(LLMType.GPT35_TURBO)\n",
    "                .set_embedding_type(EmbeddingType.BGE_SMALL_EN)\n",
    "                .set_index_type(IndexType.VECTOR_INDEX)\n",
    "                .set_ingestion_source_and_path(source=IngestionSource.HF,\n",
    "                                               paths=[[('EdinburghNLP/xsum', 'default'), ['document']]])\n",
    "                .set_retrieval_type(RetrievalType.SENTENCE_WINDOW)\n",
    "                .build()\n",
    "                )\n",
    "\n",
    "# Set up evaluator\n",
    "comp_evaluator = complete_evaluator.CompleteEvaluator(\n",
    "        query_engine=sentence_window_query_engine,\n",
    "        app_id=\"sentence_window\",\n",
    "        eval_batch=batch,\n",
    "        reset_json=True,\n",
    "        reset_csv=True\n",
    "    )\n",
    "comp_evaluator.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extractive Summarization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create query engine\n",
    "builder = RAGBuilder()\n",
    "extractive_query_engine = (builder\n",
    "                .set_llm_type(LLMType.GPT35_TURBO)\n",
    "                .set_retrieval_type(RetrievalType.EXTRACTIVE_RETRIEVAL)\n",
    "                .build()\n",
    "                )\n",
    "\n",
    "# Set up evaluator\n",
    "comp_evaluator = complete_evaluator.CompleteEvaluator(\n",
    "        query_engine=extractive_query_engine,\n",
    "        app_id=\"extractive\",\n",
    "        eval_batch=batch,\n",
    "        reset_json=False,\n",
    "        reset_csv=False\n",
    "    )\n",
    "comp_evaluator.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Topic Extraction - LDA"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create query engine\n",
    "builder = RAGBuilder()\n",
    "lda_query_engine = (builder\n",
    "                .set_llm_type(LLMType.GPT35_TURBO)\n",
    "                .set_embedding_type(EmbeddingType.BGE_SMALL_EN)\n",
    "                .set_index_type(IndexType.VECTOR_INDEX)\n",
    "                .set_ingestion_source_and_path(source=IngestionSource.HF,\n",
    "                                               paths=[[('EdinburghNLP/xsum', 'default'), ['document']]])\n",
    "                .set_retrieval_type(RetrievalType.TOPIC_EXTRACTION)\n",
    "                .set_topic_extractor_type(TopicExtractorType.LDA)\n",
    "                .set_debug_mode(True)\n",
    "                .build()\n",
    "                )\n",
    "\n",
    "# Set up evaluator\n",
    "comp_evaluator = complete_evaluator.CompleteEvaluator(\n",
    "        query_engine=lda_query_engine,\n",
    "        app_id=\"topic_lda\",\n",
    "        eval_batch=batch,\n",
    "        reset_json=False,\n",
    "        reset_csv=False\n",
    "    )\n",
    "comp_evaluator.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Topic Extraction - BERTopic"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create query engine\n",
    "builder = RAGBuilder()\n",
    "bertopic_query_engine = (builder\n",
    "                .set_llm_type(LLMType.GPT35_TURBO)\n",
    "                .set_embedding_type(EmbeddingType.BGE_SMALL_EN)\n",
    "                .set_index_type(IndexType.VECTOR_INDEX)\n",
    "                .set_ingestion_source_and_path(source=IngestionSource.LOCAL,\n",
    "                                               paths=['data/source-texts']) # As an example, here we use a local pdf.\n",
    "                .set_retrieval_type(RetrievalType.TOPIC_EXTRACTION)\n",
    "                .set_topic_extractor_type(TopicExtractorType.BERTOPIC)\n",
    "                .set_debug_mode(True)\n",
    "                .build()\n",
    "                )\n",
    "\n",
    "# Set up evaluator\n",
    "comp_evaluator = complete_evaluator.CompleteEvaluator(\n",
    "        query_engine=bertopic_query_engine,\n",
    "        app_id=\"topic_bertopic\",\n",
    "        eval_batch=batch,\n",
    "        reset_json=False,\n",
    "        reset_csv=False\n",
    "    )\n",
    "comp_evaluator.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Knowledge Graph\n",
    "\n",
    "The implementation of a knowledge-graph integration is inspired by the following three articles:<br>\n",
    "[1] [GraphIndex for RAG](https://medium.com/@nebulagraph/graph-rag-the-new-llm-stack-with-knowledge-graphs-e1e902c504ed)<br>\n",
    "[2] [Concept Graphs for RAG](https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a)<br>\n",
    "[3] [LDA for Topic Extraction](https://towardsdatascience.com/document-topic-extraction-with-large-language-models-llm-and-the-latent-dirichlet-allocation-e4697e4dae87)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create query engine\n",
    "builder = RAGBuilder()\n",
    "knowledge_graph_query_engine = (builder\n",
    "                .set_llm_type(LLMType.GPT35_TURBO)\n",
    "                .set_embedding_type(EmbeddingType.BGE_SMALL_EN)\n",
    "                .set_index_type(IndexType.KG_INDEX)\n",
    "                .set_ingestion_source_and_path(source=IngestionSource.HF,\n",
    "                                               paths=[[('EdinburghNLP/xsum', 'default'), ['document']]])\n",
    "                .set_retrieval_type(RetrievalType.KG_RETRIEVAL)\n",
    "                .set_debug_mode(True)\n",
    "                .build()\n",
    "                )\n",
    "\n",
    "# Set up evaluator\n",
    "comp_evaluator = complete_evaluator.CompleteEvaluator(\n",
    "        query_engine=knowledge_graph_query_engine,\n",
    "        app_id=\"knowledge_graph\",\n",
    "        eval_batch=batch,\n",
    "        reset_json=False,\n",
    "        reset_csv=False\n",
    "    )\n",
    "comp_evaluator.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### No Retrieval\n",
    "\n",
    "Only use large language model to generate summary. Here to show how to set up large language model using OpenAI or BART"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from app.llms.bart import BartHuggingFaceLLM\n",
    "from app.evaluation.EvaluateSummary.prompt_wrapper import with_prompt_template\n",
    "\n",
    "# Set up BART model\n",
    "bart = BartHuggingFaceLLM()\n",
    "@with_prompt_template\n",
    "def bart_completion(prompt):\n",
    "    return bart.complete(prompt).text\n",
    "\n",
    "# Set up GPT model\n",
    "@with_prompt_template\n",
    "def gpt_completion(prompt, model=\"gpt-3.5-turbo\", max_tokens=100):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a summarization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
